# 微服务

## 服务治理

微服务治理是确保微服务架构高效、稳定运行的关键环节，涉及服务管理、监控、协调和安全等多个方面。以下是常见的治理问题及其应对策略：

### 服务治理基础层

- **服务注册发现**

  - 服务启动时将自身信息（IP、端口、健康状态）注册到中心化存储库（注册中心），其他服务通过查询注册中心动态发现目标服务地址
  - 通过心跳检测，动态更新服务节点状态
  - 支持水平扩展和故障实例自动剔除

- **配置中心**

  - 统一管理多环境配置（Dev/Test/Prod）
  - 支持灰度发布和实时热更新
  - 配置变更日志、数据加密存储、权限管控

- **服务部署**

  - 支持服务分阶段部署
  - 支持弹性扩缩容

### 可用性保障

- **基础通信保障**

  - 超时控制：防止线程阻塞问题，提供超时时间自适应调整能力
  - 重试策略：指数退避算法、最大重试次数
  - 连接池管理：动态调整最大连接数、回收空闲连接

- **异步通信**

  - 通过消息队列实现削峰填谷
  - 消息可靠性：事务消息、消费者 ack 机制、死信队列处理
  - 最终一致性方案：本地消息表、saga 事务模式

- **流量调控**

  - 熔断：当服务调用失败率超过阈值时，自动切断请求并快速失败，防止级联故障
    - 侧重点在于链路上某个服务异常，需要考虑如何自动恢复
  - 降级：资源有限的场景下，优先保障核心场景的可用性
    - 非核心场景返回兜底数据等
    - 降级是从服务负载考虑，熔断是降级的一种情况
  - 限流：对特定时间窗口内的并发量进行限制
    - 达到限制后，根据业务场景，可采取拒绝服务、排队等待、服务降级等策略
  - 多环境管理：管理开发、测试、预发、生产等环境的配置与隔离

- **负载均衡**

  - 根据服务实例清单，结合客户端（Ribbon）或服务端（Nginx）负载策略实现流量分发
  - 优化服务资源利用率，提高吞吐量和可用性，避免单点过载
  - RPC 调用场景下，通常由框架支持，其他场景由网关支持

### 可观测性体系

- **链路追踪**
  - 通过 TraceID 记录请求在分布式系统中的完整调用链路
  - 用于性能分析和故障排查
  - 生成服务依赖拓扑

- **日志聚合**
  - 集中收集、存储和检索分散在多节点上的日志数据
  - 关联 TraceID 统一日志视图，支持快速定位问题

- **监控告警**
  - 实时采集系统指标（如CPU、内存、请求成功率），及时预警异常
  - 指标层级：
    - 基础设施监控：服务器资源，如 CPU、Memory、GC
    - 服务监控：接口响应时间、错误率等
    - 业务监控：订单量、支付成功率等自定义指标
  - 核心数据可视化（Grafana）

### API 网关层

- **统一入口**
  - 路由转发（Path匹配）、协议转换（gRPC-HTTP）、跨域处理

- **安全治理**
  - 认证：OAuth2 鉴权、JWT 令牌校验
  - 防护：WAF 防护（SQL 注入、XSS 检测）、DDoS 防护
  - 数据安全：针对核心参数，加密或脱敏

- **提供治理能力**
  - 提供 API 级别的超时、重试、降级、限流等能力
  - 支持负责均衡能力

### RPC 框架

- **通信抽象**
  - 屏蔽远程调用细节（如TCP/UDP、连接池管理、服务发现、负载均衡）
  - 通过本地方法调用模式实现跨进程通信

- **协议编解码**
  - 定义数据序列化协议（如 Protobuf/Thrift），实现高效二进制传输，降低网络带宽消耗
  - 支持跨语言交互，通过 IDL 统一定义接口，确保异构系统兼容性

- **集成治理能力**
  - 内置流量控制（限流/熔断）、容错机制（重试策略）、超时控制等治理能力
  - 与注册中心、配置中心、链路追踪深度集成
  - 支持元数据在链路上透传

- **性能优化**
  - 基于长连接复用、传输压缩、零拷贝等技术提升通信效率
  - 采用 NIO/Epoll 等高性能网络模型，优化线程池管理，降低上下文切换开销

### Service Mesh

Service Mesh（服务网格）是专用于处理服务间通信的基础设施层，以透明代理（Sidecar）模式嵌入到应用网络栈中，实现流量管理、安全、可观测性等能力，与业务代码完全解耦。

- **核心架构**

  - 控制平面：统一管理代理配置，提供服务发现、策略下发、证书管理
  - 数据平面：负责流量转发与策略执行，由 Sidecar 代理组成

  ```plaintext
  应用容器 → Sidecar代理（拦截出入流量） → 目标服务Sidecar → 目标应用容器
  ```

- **实现方案**

  - Sidecar 注入
    - 注入方式：通过 Kubernetes MutatingWebhook 自动为 Pod 注入 Sidecar 容器
    - 流量劫持：使用 iptables 或 eBPF 重定向流量到 Sidecar 代理
    - 多语言支持：Sidecar 代理独立运行，与业务无关

  - 通信协议处理
    - 协议嗅探：自动识别 HTTP/1.1、HTTP/2、gRPC 等协议
    - 自定义协议支持：通过声明端口协议（如Dubbo、Thrift）实现非 HTTP 流量的治理

  - 配置下发机制
    - xDS：数据平面通过 xDS 协议访问控制平面，动态获取 Listener、Cluster、Route、Filter 等配置。  

    ```plaintext
    数据平面 ↔ gRPC长连接 ↔控制平面
    ```

- **核心功能**
  - 服务发现：
    - 新的服务实例启动时，会向控制平面注册自己的信息
    - Sidecar 与控制中心交互，获取全局信息
  - 流量调度：
    - 流量完全由 Sidecar 进行代理
    - 支持动态路由、负载均衡、故障注入、熔断、限流、超时、重试等能力
  - 服务安全：
    - 服务间通信引入 mTLS 协议、加密处理
    - 支持服务、接口等细粒度的权限控制
  - 可观测性：
    - 自动注入 TraceID，实现分布式链路追踪
    - 收集 Sidecar 相关日志，实时监控服务间流量、延迟、错误率等指标

- **核心能力**

### 模块间协同工作示例

场景：大促期间商品秒杀活动

1. **流量入口阶段**  
   - API网关启用限流规则（每秒5000请求），超出部分直接返回"活动火爆"提示页  
   - 网关验证用户Token有效性，拦截恶意爬虫请求（安全治理）

2. **核心服务调用阶段**  
   - 订单服务通过注册中心获取库存服务3个实例，Ribbon按轮询策略发起调用  
   - 设置300ms超时时间，库存服务响应延迟超过阈值时触发熔断，降级为本地库存缓存  
   - 扣减库存成功后，通过Kafka异步通知物流服务生成运单（最终一致性）

3. **异常处理阶段**  
   - 支付服务调用银行接口失败，根据重试策略进行3次间隔重试（1s/3s/5s）  
   - 持续失败则触发Saga补偿，回滚订单状态并释放库存

4. **监控与诊断阶段**  
   - SkyWalking显示库存服务平均响应时间从50ms上升至400ms，触发自动扩容（K8s增加2个Pod）  
   - Grafana检测到支付服务错误率超过5%，推送企业微信告警通知  
   - 通过TraceID在Kibana中检索到支付超时的根本原因是第三方接口证书过期
